\documentclass[twocolumn]{article}
\usepackage{cite}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{url}
\title{OmniVisor - OmniScale Hypervisor for Disaggregated Architecture}
\author{Yu-Ju Huang}
\date{Cornell University}
\begin{document}
\maketitle

\begin{abstract}
Put your abstract here.
\end{abstract}

\iffalse
\begin{figure*}[h!]
  \subcaptionbox{Monolithic architecture, hardware components are bounded within a machine.\label{fig:monolithic}}[.3\textwidth]{\includegraphics[height=.2\textheight,width=.3\textwidth]{figures/monolithic.png}}
  \subcaptionbox{Disaggregated architecture, hardware components are separate: there are CPU-blade, memory-blade, storage-blade. We also assume there is local memory located with CPU.\label{fig:disaggregated}}[.7\textwidth]{\includegraphics[height=.2\textheight,width=.7\textwidth]{figures/disaggregated.png}}
  \caption{Monolithic architecture and disaggregated architecture.}
  \label{fig:architecture}
\end{figure*}
\fi

% Figure of monolithic architecture and disaggregared architecture
\begin{figure*}[h!]
     \centering
     \captionsetup[subfigure]{position=b}
     \begin{subfigure}[b]{0.28\textwidth}
         \includegraphics[height=.2\textheight]{figures/monolithic.png}
         \caption{Monolithic architecture, hardware components are bounded within a machine.}
         \label{fig:monolithic architecture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.68\textwidth}
         \includegraphics[height=.2\textheight]{figures/disaggregated.png}
         \caption{Disaggregated architecture, hardware components are separate: there are CPU-blade, memory-blade, storage-blade. We also assume there is local memory located with CPU}
         \label{fig:disaggregated architecture}
     \end{subfigure}
     \caption{Monolithic architecture and disaggregated architecture.}
     \label{fig:architecture}
\end{figure*}

\section{Introduction}

The cloud computing has been popular since 2006. Recently, there are growing interests in investigating the limits IaaS (Infrastructure as a Service) has faced. The IaaS is built on top of many servers and managed by a cloud operating system.  Cloud operating system accepts demands from users and allocate resources to them. While the interest in adopting this services is still growing, resource utilization of the CPU and memory is usually not full. The reason behind this under-utilization is that there is always segment issues for allocating CPU and memory. Furthermore, the servers underpinning the IaaS is not a good fit for raising trend of heterogeneous computing as adding various new computing devices into servers is not a trivial process. \par
    Hardware resource disaggregated architecture is proposed to address these limits. In this architecture, resources are no longer bound with a server. In other words, resources are collected in pools, such as processor pool, memory pool, and storage pool, and there is no boundary among each class of resource. In this setting, the resources can be distributed in the exact number of requested, the unallocated resources can be shutdown. \par
    While there is an operating system prototype, LegoOS, designed for the disaggregated architecture. We believe the main purpose of disaggregated architecture is to support cloud computing. Designing a hypervisor to support the IaaS functionality is therefore an important thing to be done. \par
    OmniVisor is a hypervisor in omni scale, for both aggregated and disaggregated architecture, and the goal is to address the limits of IaaS mentioned above. We foresee the practicable disaggregated architecture will not be complete in the recent years, so considering a hypervisor for aggregated architecture is also important. In supporting aggregated architecture, the difference between OmniVisor and previous hypervisors, such as Xen, KVM, and VMware’s products, are where does the computation happen. For previous hypervisors, all the computation is carried out within a server, i.e., an aggregated machine, while for OmniVisor, it makes computation happen across machines, i.e., cross-boundary computation. For example, OmniVisor is able to allocate processors from two servers and give them to a user. This resources separation is transparent to users, they still view it as a machine abstraction and use an operating system (guest operating system from the hypervisor’s point of view) to manage it. \par
    OmniVisor is a general solution to address limits of IaaS. It can be divided into two parts: support for aggregated and disaggregated architecture. It is certain that designing a hypervisor for disaggregated architecture will be an impactful work. As for the aggregated part, we should ask one question first: can the cross-boundary computation outperform current mechanism? For current design, even though the requested resources are more than the underlying machine can support, the hypervisor can still give out virtual resources by using time-sharing scheduler and storage to simulate processors and memory respectively. So we must first answer the benefits of cross-boundary computation in comparing with the previous design. \par
    This project will focus on the feasibility of OmniVisor for aggregated architecture. I will implement a prototype and evaluate the performance comparing with current hypervisors. Or at least, evaluate the performance requirement for designing the OmniVisor. For instance, since the cross-boundary computation requires communication through network, the network performance requirement for outperforming current hypervisors will be investigated. \par

\section{Background}

\emph{Poor hardware elastisity}

\emph{Poor failure handling}


\subsection{Resource Disaggregation}


\begin{itemize}
\item \texttt{pdflatex latex}
\end{itemize}

\section{Related Work}

The datacenter systems have used server-centric architecture for many years where the hardware resources are tightly integrated within one machine. There are some limits on the server-centric architecture including under-utilized resources, weak support for heterogeneous computing, poor failure handling, and hard to achieve hardware resource elasticity. The resource disaggregation architecture is then proposed to tackle these limits. There are some disaggregated architecture proposals include Intel’s Rack Scale Architecture[1], HP’s “The Machine”[2], Facebook’s Disaggregated Rack[3], and IBM’s Composable System[4]. The key enabling factor of the disaggregated architecture is network techniques that whether network performance can meet the performance requirement of bus-connected processors and memory[5]. There are some network techniques enable the possibility of the disaggregated architecture: the high-speed and more scalable network techniques such as RDMA[6], InfiniteBand[7], and new network stack for rack-scale computers[8]; network hardware devices accessing without the need of attaching to processors[9, 10]. \par
    As for the software sides to manage resources on disaggregated architecture, there are some techniques to target specific types of resources. Support disaggregated memory with transparently access through processor’s memory instructions[11, 12]. Another challenging point to resource disaggregation is how to separate memory from processors as most of the architecture have tightly-coupled processors and memory. Moreover, there are also instructions carrying out memory access. LegoOS[18] emulates the separation of processor and memory by trapping all memory accesses, excluding cache access, and redirect them to access remote memory via page fault handler. As for storage management, accessing the remote disks is already a common practice [13, 14]. \par
    For operating systems, there are some previous works to manage the distributed resource across multiple machines. Ameoba OS[15, 16] and fos[17] builds a single image microkernel-based OS to run across multiple physical machines, for example, supporting Infrastructure as a Service (IaaS) in the cloud computing context. The motivation is to release application developers from the complexity of resources management. Having many machines to form a huge resource pool, users should be able to use the resources without considering managing them across machines or worry about the location of machines. For example, in the cloud computing context, developers don’t need to consider how many resources and VMs are required for a certain task. \par
    Both Ameoba OS and fos support allocate resources from multiple machines. One major difference between these two is that, though Ameoba OS supports allocate resources from many machines, once the machine is selected, the process will only use the hardware components on it, e.g., the process cannot use processors and memory on two machines.  fos, on the other hand, does allow one application to use resources from different machines. fos provides the resource elasticity functionality by allowing users to simply call a spawn procedure. The machine where the spawned process runs on, either on the same machine or on a different machine, is handled by the fos. \par
    Though OmniVisor shares the same goal of Ameoba OS and fos in exposing many machines as a single machine abstraction to users, there are two primary differences. First, OmniVisor considers disaggregated resources, where the processors, memory, and storage are decoupled. This adds one technical challenge for OS/hypervisor design to manage the separated resources. The separation of processors and memory is especially a difficult issue. Second, OmniVisor is a layer between OS and underlying hardware components. This provides more flexibility for users to choose different OSes to use. Moreover, it provides full backward compatibility for the existing OSes and applications. \par
    LegoOS[18] is a prototype operating system specifically targeting on disaggregated architecture. It proposes a splitkernel OS architecture that split the kernel functionalities into several services. As the hardware resources are separate, the managing software is also split. Each service manages its own hardware component and communicates with each other via network. For instance, there are different software components to manage processors, memory, and storage hardware individually. But same as Ameoba OS and fos, LogoOS design a completely new OS and therefore is weak in backward compatibility and lack of flexibility for users to choose different OSes to use. \par

% Figure of local memory architecture
\begin{figure*}[h!]
     \centering
     \captionsetup[subfigure]{position=b}
     \begin{subfigure}[b]{\textwidth}
         \includegraphics[height=.05\textheight,width=\linewidth]{figures/memory_region.png}
         \caption{Separate physical main memory into local (slash-filled) and remote memory resgions.}
         \label{fig:local memory regions}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\textwidth}
         \includegraphics[height=.1\textheight,width=\linewidth]{figures/shared_local_mem.png}
         \caption{The four VMs, blue colered, share the local memory region. It's possible that guest physical address of two VMs end up mapping to the same machine physical address, VM2 and VM3 for example.}
         \label{fig:shared local memory}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\textwidth}
         \includegraphics[height=.1\textheight,width=\linewidth]{figures/dedicated_local_mem.png}
         \caption{The four VMs, blue colered own its own local memory region. These regions are not shared with other VMs.}
         \label{fig:dedicated local memory}
     \end{subfigure}
     \caption{Configuration of local memory region.}
     \label{fig:local memory architecture}
\end{figure*}

\section{Design}
In this section, we first discuss the application model. In particular, do applications or guest OSes know there are running on top of disaggregated architecture? After introducing the application model, our simulation approach is described.

\subsection{Application Model}
One of our primary reasons for building a hypervisor for disaggregated architecture is it provides the benefits for legacy applications and guest OSes. The softwares running one traditional architecture are not expected to make any modification. With the hypervisor, we look forward to more adoption of disaggregated architecture. Moreover, we think it's the systems that need to adapt the architecture changes rather than applications. Our hypervisor can also guide the disaggregated hardware design, especially the resource disaggregation is in a very beginning stage.

\subsection{Target Architecture}
For disaggregated architecture, all hardware computing resources such as processors, memory, and storage are separately located. Though processors and memory are separate, we assume there are still a few amount of memory stay with the processors. This assumption is reasonable given the network speed still not catches on the processor-memory performance requirement\cite{NetworkRequirement} and datacenter applications usually show great temporal data locality\cite{NNLoad,MapReduceLoad}. So in this paper, the memory components in our target architecture include local and remote memory as depicted in Fig \ref{fig:disaggregated architecture}.

We simulate the local and remote memory using the main memory of a machine as illustrated in Fig \ref{fig:local memory regions}. We manage the page table to separate the main memory as for simulating local and remote memory on the disaggregated architecture. Only the memory address to the local memory region is valid, accessing other address will cause a page fault. In the page fault handler, we will first check whether the page stays on the remote memory region and copy the data into local memory region if so. Otherwise, a storage access for reading the data into remote memory region, and then copy the data into local memory region. Local page eviction, page table entry invalidation, and TLB invalidation will also be executed to guarantee correctness. Moreover, we will execute a intentional wait in the page fault handler to simulate the latency for fetching a remote memory via network in the disaggregated architecture. The waiting time is a configurable variable in order to see in which level of the latency is acceptable for achieving comparable performance with monolithic architecture.

\subsection{Dedicated Local Memory}
In this paper, we explore the possibility of providing dedicated local memory to each virtual machine and compare it with the the shared local memory policy to see which is a better policy for disaggregated architecuture. See Fig \ref{fig:shared local memory} and Fig \ref{fig:dedicated local memory} for illustrations.

For shared local memory, we simply make the whole local memory region shared by multiple virtual machines and use the original page replacement policy implemented in the hypervisor to choose the victim to be evicted. So the victim page might be a page of any virtual machine.

Dedicated local memory, on the other hand, we divided the whole local memory region and assign each sub-region to each virtual machine equally. Whenever a page needs to be evicted, only the page in the sub-region owned by the virtual machine causing the page fault will become a victime. The process will not evict a page in other virtual machines' local memory region.

\section{Implementation}

\section{Evaluation}
In the evaluation section, we try to answer two questions: 1) how many local memory is required to prevent significant performance drop? 2) should the local memory be dedicated or shared between guest OSes? After understanding these two questions, we can make a hint about how a hypervisor does local memory management for disaggregated architecture.

For the first question, the purpose is to know whether the disaggregated architecture can still achieve comparable performance even most of the memory stay remotely. We run workloads on one virtual machine once at a time and vary the local memory size for each run. The baseline is to run the workloads using all memory as local memory. We try to find one size setting that can achieve ~90\% of the baseline performance.

As for the second question, we try to know that if local memory is not enough for multiple virtual machines, will using shared memory compromise the performance isolation between virtual machines too much. And whether using dedicated memory cause under-utilization issue. For the experiment, we run workloads on multiple virtual machines. The primary control variable is enable shared or dedicated memory and to observe the performance variation.

%latex here should be the name of your bibtex file minus '.bib'
\bibliography{paper}{}
\bibliographystyle{plain}

\end{document}
