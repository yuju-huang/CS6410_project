\documentclass[twocolumn]{article}
\usepackage{cite}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{url}
\title{Explore Local Memory Design for Building Hypervisor on Resource Disaggregated Architecture}
\author{Yu-Ju Huang}
\date{Cornell University}
\begin{document}
\maketitle

\begin{abstract}
Resource disaggregation is a new trend for building datacenter as it solves many limitations of traditional monolithic server architecture. Building a hypervisor can provide full compability to enable legacy applications to run on disaggregated architecture. Furthermore, virtualization functionality is usually an important feature a datacenter should possess, to support Infrastructure as a Service (IaaS) for instance.

This work explores the local memory design choices for building a hypervisor on disaggregated architecture. As in disaggregated architecture, main memory stay remotely instead of locating with processors, the management of local memory will be the key system performance. We try to know how many local memory is enough for running a virtual machine to achieve comparable performance in monolithic architecture. Another unclear thing is that, for running multiple virtual machines, should the local memory be dedicated or shared.

\end{abstract}

% Figure of monolithic architecture and disaggregared architecture
\iffalse
\begin{figure*}[h!]
  \subcaptionbox{Monolithic architecture, hardware components are bounded within a machine.\label{fig:monolithic}}[.3\textwidth]{\includegraphics[height=.2\textheight,width=.3\textwidth]{figures/monolithic.png}}
  \subcaptionbox{Disaggregated architecture, hardware components are separate: there are CPU-blade, memory-blade, storage-blade. We also assume there is local memory located with CPU.\label{fig:disaggregated}}[.7\textwidth]{\includegraphics[height=.2\textheight,width=.7\textwidth]{figures/disaggregated.png}}
  \caption{Monolithic architecture and disaggregated architecture.}
  \label{fig:architecture}
\end{figure*}
\fi

\begin{figure*}[h!]
     \centering
     \captionsetup[subfigure]{position=b}
     \begin{subfigure}[b]{0.28\textwidth}
         \includegraphics[height=.2\textheight]{figures/monolithic.png}
         \caption{Monolithic architecture, hardware components are bounded within a machine.}
         \label{fig:monolithic architecture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.68\textwidth}
         \includegraphics[height=.2\textheight]{figures/disaggregated.png}
         \caption{Disaggregated architecture, hardware components are separate: there are CPU-blade, memory-blade, storage-blade. We also assume there is local memory located with CPU}
         \label{fig:disaggregated architecture}
     \end{subfigure}
     \caption{Monolithic architecture and disaggregated architecture.}
     \label{fig:architecture}
\end{figure*}

\section{Introduction}

The cloud computing has been popular since 2006. Recently, there are growing interests in investigating the limits IaaS (Infrastructure as a Service) has faced. The IaaS is built on top of many servers and managed by a cloud operating system.  Cloud operating system accepts demands from users and allocate resources to them. While the interest in adopting this services is still growing, resource utilization of the CPU and memory is usually not full. The reason behind this under-utilization is that there is always segment issues for allocating CPU and memory. Furthermore, the servers underpinning the IaaS is not a good fit for raising trend of heterogeneous computing as adding various new computing devices into servers is not a trivial process.

Hardware resource disaggregated architecture is proposed to address these limits. In this architecture, resources are no longer bound with a server. In other words, resources are collected in pools, such as processor pool, memory pool, and storage pool, and there is no boundary among each class of resource. In this setting, the resources can be distributed in the exact number of requested, the unallocated resources can be shutdown. For the following article, we use monolithic architecture as the traditional server design and disaggregated architecture as our target architecture, as illustrated in Fig \ref{fig:architecture}.

While there is an operating system prototype, LegoOS\cite{LegoOS}, designed for the disaggregated architecture. We believe the main purpose of disaggregated architecture is to support cloud computing. Designing a hypervisor to support the IaaS functionality is therefore an important thing to be done.

For disaggregated architecture, all hardware computing resources such as processors, memory, and storage are separately located. Though processors and memory are separate, we assume there are still a few amount of memory stay with the processors. This assumption is reasonable given the network speed still not catches on the processor-memory performance requirement\cite{Network_requirement} and datacenter applications usually show great temporal data locality\cite{NN_load,MapReduce_load}. So in this paper, the memory components in our target architecture include local and remote memory as depicted in Fig \ref{fig:disaggregated architecture}.

This work is a pre-step before going into the design of hypervisor as there are some unclear questions about local memory on disaggregated architecture needed to be answered. We focus on local memory since it makes huge impact in the disaggregated architecture as the main memory stay remotely and has higher latency. First thing we explore is that, how many local memory is enough for running a virtual machine and to achieve comparable performance? Second, for running multiple virtual machines, should the local memory be dedicated or shared? This work uses a simulation approach to evaluate the performance of applications and try to answer the above questions.

We simulate the disaggregated architecutre by separating the main memory of a machine into local and remote memory regions. The processes are only allowed to access local memory region. page fault and page eviction happen if the data is not in local memory region. The data is first loaded into remote memory region and then is copied to local memory region before following access. A configurable delay is added in the page fault handler to simulate the latency in access remote memory via network.

To sum up, we believe building a hypervisor for disaggregated architecture is a more practical way to explore it. And in the virtualized environment, this paper explore what should be the right local memory design to achieve acceptable performance comparing with traditional monolithic architecture.

\section{Background}

In the traditional architecture, programs (for destop users) or virtual machine (as what IaaS provides) run on top of a monolithic machine where all of the hardware components are bounded within the machine.  While this setting has been working well for many years, current trends raise some distributeculties to adopt this model. Beside, there are also new techiniques to be enabling factors of disaggregated architecture.  The following sections discuss the limitation of monolithic servers and then descripe the primary concept of resource disaggregation.

\subsection{Monolithic Architecture Limitations}
In this subsection, we discuss about three limitations of monolithic architecture: rosource under-utilization, poor hardware elasticity, and poor failure handling.

First, in monolithic servers, all the hardware components are bounded within a physical machine. In the context of IaaS, for example, these machine boundaries are not a good fit for providing virtual machines to users. Think about a scenario that an user requests a virtual machine composed of many processors but only a few amount of memory. This request ends up using most of the processors but only a small portion of memory in a physical machine. It's then not possible for cloud OS to distribute other users' request to that physical machine as there are not many processors left. Therefore, the memory of the machine is under-utilized. This under-utilization issues could be generalized into any categoriy of hardware resources.

As for poor hardware elastisity, having boundary on hardware resources makes it extremely hard to add or remove hardware components to a machine. Increasing the server's computation power via adding more processors or memory is not easy to do. Furthermore, this limitation becomes more daunting as specific hardware accerlerators are viewed as a future trend these days\cite{Heterogeous_computing}, such as GPGPU\cite{Google_GPGPU}, TPU\cite{TPU}, FPGA\cite{Datacenter_FPGA}. While in disaggregated setting, any hardware component could be add or remove via attach or de-attach the HW-blade to other inter-connected hardware components.

Finally, the failure handling for monolithic architecure is that, even though there is a failure on only one hardware component, that failure usually causes entire system crushes. This is not an appealing outcome as people are expecting hardware component failures could be handled independently without breaking down the whole system. In the cloud provider, this failure model is even severe since the crush of entire machine will affect multiple users. In the disaggregated architecture, however, the faults of different components could be handled independently as there are loose-coupled.

\subsection{Resource Disaggregation}

The concept of resource disaggregation is that instead of having hardware components bounded within a physical machine, different hardware components should be separate. We could have processors, memory, storages, and I/O devices sit in different place. These hardware components communicate with each other to provide functionalities that a computer should provide. From a user's persepctive, it just sees a machine abstraction carrying out all their computing requests. 

Providing machine abstraction by separate hardware components face some technical challenges\cite{Rack_scale_challenges}. The first one is the efficiency of network communication. Performance degradation can be expected if the network speed is not fast enough. Another chanllenge is that traditional hardware components cannot work independently and might require some assistance from CPUs. Take DMA for example, though DMA is free of CPU's intervention during data copy, it still needs CPU to do some initialization. The cases are same in network and storage devices.

Fortunately, current techniques resolve some of the above challenges. We descipe some key enabling factors for resource disaggregation. First, faster network communication\cite{R2C2}, RDMA\cite{RDMA}, and InfiniteBand\cite{InfiniteBand} are introduced and becomes more and more popular recently. Furthermore, hardware devices with programmablity of standalone computing funcationality is also emerging\cite{PARDIS, PIM}.

\section{Related Work}

The datacenter systems have used server-centric architecture for many years where the hardware resources are tightly integrated within one machine. There are some limits on the server-centric architecture including under-utilized resources, weak support for heterogeneous computing, poor failure handling, and hard to achieve hardware resource elasticity. The resource disaggregation architecture is then proposed to tackle these limits. There are some disaggregated architecture proposals include Intel’s Rack Scale Architecture\cite{Intel_RSA}, HP’s “The Machine”\cite{HP_The_Machine}, Facebook’s Disaggregated Rack\cite{FB_disaggregated_rack}, and IBM’s Composable System\cite{IBM_composable_system}. The key enabling factor of the disaggregated architecture is network techniques that whether network performance can meet the performance requirement of bus-connected processors and memory\cite{Network_requirement}. There are some network techniques enable the possibility of the disaggregated architecture: the high-speed and more scalable network techniques such as RDMA\cite{RDMA}, InfiniteBand\cite{InfiniteBand}, and new network stack for rack-scale computers\cite{Network_stack_for_rack}; network hardware devices accessing without the need of attaching to processors\cite{NVM_performance, NVM_performance2}. \par
    As for the software sides to manage resources on disaggregated architecture, there are some techniques to target specific types of resources. Support disaggregated memory with transparently access through processor’s memory instructions\cite{Disaggregated_memory, Disaggregated_memory2}. Another challenging point to resource disaggregation is how to separate memory from processors as most of the architecture have tightly-coupled processors and memory. Moreover, there are also instructions carrying out memory access. LegoOS[18] emulates the separation of processor and memory by trapping all memory accesses, excluding cache access, and redirect them to access remote memory via page fault handler. As for storage management, accessing the remote disks is already a common practice\cite{VMware_virtual_SAN, Amazon_EC2_root_volume}. \par
    For operating systems, there are some previous works to manage the distributed resource across multiple machines. Ameoba OS\cite{Amoeba_OS} and fos\cite{fos} builds a single image microkernel-based OS to run across multiple physical machines, for example, supporting Infrastructure as a Service (IaaS) in the cloud computing context. The motivation is to release application developers from the complexity of resources management. Having many machines to form a huge resource pool, users should be able to use the resources without considering managing them across machines or worry about the location of machines. For example, in the cloud computing context, developers don’t need to consider how many resources and VMs are required for a certain task. \par
    Both Ameoba OS and fos support allocate resources from multiple machines. One major difference between these two is that, though Ameoba OS supports allocate resources from many machines, once the machine is selected, the process will only use the hardware components on it, e.g., the process cannot use processors and memory on two machines.  fos, on the other hand, does allow one application to use resources from different machines. fos provides the resource elasticity functionality by allowing users to simply call a spawn procedure. The machine where the spawned process runs on, either on the same machine or on a different machine, is handled by the fos. \par
    Though OmniVisor shares the same goal of Ameoba OS and fos in exposing many machines as a single machine abstraction to users, there are two primary differences. First, OmniVisor considers disaggregated resources, where the processors, memory, and storage are decoupled. This adds one technical challenge for OS/hypervisor design to manage the separated resources. The separation of processors and memory is especially a difficult issue. Second, OmniVisor is a layer between OS and underlying hardware components. This provides more flexibility for users to choose different OSes to use. Moreover, it provides full backward compatibility for the existing OSes and applications. \par
    LegoOS\cite{LegoOS} is a prototype operating system specifically targeting on disaggregated architecture. It proposes a splitkernel OS architecture that split the kernel functionalities into several services. As the hardware resources are separate, the managing software is also split. Each service manages its own hardware component and communicates with each other via network. For instance, there are different software components to manage processors, memory, and storage hardware individually. But same as Ameoba OS and fos, LogoOS design a completely new OS and therefore is weak in backward compatibility and lack of flexibility for users to choose different OSes to use. \par

% Figure of local memory architecture
\begin{figure*}[h!]
     \centering
     \captionsetup[subfigure]{position=b}
     \begin{subfigure}[b]{\textwidth}
         \includegraphics[height=.05\textheight,width=\linewidth]{figures/memory_region.png}
         \caption{Separate physical main memory into local (slash-filled) and remote memory resgions.}
         \label{fig:local memory regions}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\textwidth}
         \includegraphics[height=.1\textheight,width=\linewidth]{figures/shared_local_mem.png}
         \caption{The four VMs, blue colered, share the local memory region. It's possible that guest physical address of two VMs end up mapping to the same machine physical address, VM2 and VM3 for example.}
         \label{fig:shared local memory}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\textwidth}
         \includegraphics[height=.1\textheight,width=\linewidth]{figures/dedicated_local_mem.png}
         \caption{The four VMs, blue colered own its own local memory region. These regions are not shared with other VMs.}
         \label{fig:dedicated local memory}
     \end{subfigure}
     \caption{Configuration of local memory region.}
     \label{fig:local memory architecture}
\end{figure*}

\section{Design}
In this section, we first discuss the application model. In particular, do applications or guest OSes know there are running on top of disaggregated architecture? After introducing the application model, our simulation approach is described.

\subsection{Application Model}
One of our primary reasons for building a hypervisor for disaggregated architecture is it provides the benefits for legacy applications and guest OSes. The softwares running one traditional architecture are not expected to make any modification. With the hypervisor, we look forward to more adoption of disaggregated architecture. Moreover, we think it's the systems that need to adapt the architecture changes rather than applications. Our hypervisor can also guide the disaggregated hardware design, especially the resource disaggregation is in a very beginning stage.

\subsection{Simulation Approach}
We simulate the local and remote memory using the main memory of a machine as illustrated in Fig \ref{fig:local memory regions}. We manage the page table to separate the main memory as for simulating local and remote memory on the disaggregated architecture. Only the memory address to the local memory region is valid, accessing other address will cause a page fault. In the page fault handler, we will first check whether the page stays on the remote memory region and copy the data into local memory region if so. Otherwise, a storage access for reading the data into remote memory region, and then copy the data into local memory region. Local page eviction, page table entry invalidation, and TLB invalidation will also be executed to guarantee correctness. Moreover, we will execute a intentional wait in the page fault handler to simulate the latency for fetching a remote memory via network in the disaggregated architecture. The waiting time is a configurable variable in order to see in which level of the latency is acceptable for achieving comparable performance with monolithic architecture. Also, the size of local memory region is changeable in order to do analysis on what local memory size is a better design for disaggregated architecture.

\subsection{Shared/Dedicated Local Memory}
In this paper, we explore the possibility of providing dedicated local memory to each virtual machine and compare it with the the shared local memory policy to see which is a better policy for disaggregated architecuture. See Fig \ref{fig:shared local memory} and Fig \ref{fig:dedicated local memory} for illustrations.

For shared local memory, we simply make the whole local memory region shared by multiple virtual machines and use the original page replacement policy implemented in the hypervisor to choose the victim to be evicted. So the victim page might be a page of any virtual machine.

Dedicated local memory, on the other hand, we divided the whole local memory region and assign each sub-region to each virtual machine equally. Whenever a page needs to be evicted, only the page in the sub-region owned by the virtual machine causing the page fault will become a victime. The process will not evict a page in other virtual machines' local memory region.

\section{Implementation}

\section{Evaluation}
In the evaluation section, we try to answer two questions: 1) how many local memory is required to prevent significant performance drop? 2) should the local memory be dedicated or shared between guest OSes? After understanding these two questions, we can make a hint about how a hypervisor does local memory management for disaggregated architecture.

For the first question, the purpose is to know whether the disaggregated architecture can still achieve comparable performance even most of the memory stay remotely. We run workloads on one virtual machine once at a time and vary the local memory size for each run. The baseline is to run the workloads using all memory as local memory. We try to find one size setting that can achieve ~90\% of the baseline performance.

As for the second question, we try to know that if local memory is not enough for multiple virtual machines, will using shared memory compromise the performance isolation between virtual machines too much. And whether using dedicated memory cause under-utilization issue. For the experiment, we run workloads on multiple virtual machines. The primary control variable is enable shared or dedicated memory and to observe the performance variation.

%latex here should be the name of your bibtex file minus '.bib'
\bibliography{paper}{}
\bibliographystyle{plain}

\end{document}
