\documentclass[twocolumn]{article}
\usepackage{cite}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{url}
\title{Explore Local Memory Design for Building Hypervisor on Resource Disaggregated Architecture}
\author{Yu-Ju Huang}
\date{Cornell University}
\begin{document}
\maketitle

\begin{abstract}
Resource disaggregation is a new trend for building datacenter as it solves many limitations of traditional monolithic server architecture. Building a hypervisor can provide full compatibility to enable legacy applications to run on disaggregated architecture. Furthermore, virtualization functionality is usually an important feature a datacenter should possess, to support Infrastructure as a Service (IaaS) for instance.

This work explores the local memory design choices for building a hypervisor on disaggregated architecture. As in disaggregated architecture, the main memory stays remotely instead of locating with processors, the management of local memory will be the key system performance. We try to know how much local memory is enough for running a virtual machine to achieve comparable performance in a monolithic architecture. Another unclear thing is that, for running multiple virtual machines, should the local memory be dedicated or shared.

\end{abstract}

% Figure of monolithic architecture and disaggregared architecture
\iffalse
\begin{figure*}[h!]
  \subcaptionbox{Monolithic architecture, hardware components are bounded within a machine.\label{fig:monolithic}}[.3\textwidth]{\includegraphics[height=.2\textheight,width=.3\textwidth]{figures/monolithic.png}}
  \subcaptionbox{Disaggregated architecture, hardware components are separate: there are CPU-blade, memory-blade, storage-blade. We also assume there is local memory located with CPU.\label{fig:disaggregated}}[.7\textwidth]{\includegraphics[height=.2\textheight,width=.7\textwidth]{figures/disaggregated.png}}
  \caption{Monolithic architecture and disaggregated architecture.}
  \label{fig:architecture}
\end{figure*}
\fi

\begin{figure*}[h!]
     \centering
     \captionsetup[subfigure]{position=b}
     \begin{subfigure}[b]{0.28\textwidth}
         \centering
         \includegraphics[height=.15\textheight]{figures/monolithic.png}
         \caption{Monolithic architecture, hardware components are bounded within a machine.}
         \label{fig:monolithic architecture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.68\textwidth}
         \centering
         \includegraphics[height=.15\textheight]{figures/disaggregated.png}
         \caption{Disaggregated architecture, hardware components are separate: there are CPU-blade, memory-blade, storage-blade. We also assume there is local memory located with CPU}
         \label{fig:disaggregated architecture}
     \end{subfigure}
     \caption{Monolithic architecture and disaggregated architecture.}
     \label{fig:architecture}
\end{figure*}

% Figure for memory hierarchy
\begin{figure}[h!]
     \centering
     \includegraphics[width=\linewidth,height=.15\textheight]{figures/memory_hierarchy.png}
     \caption{Memory hierarchy of disaggregated architecture.}
     \label{fig:memory hierarchy}
\end{figure}

\section{Introduction}
Cloud computing has been popular since 2006. Recently, there are growing interests in investigating the limits IaaS (Infrastructure as a Service) has faced. The IaaS is built on top of many servers and managed by a cloud operating system. Cloud operating system accepts demands from users and allocates resources to them. While the interest in adopting these services is still growing, resource utilization of the CPU and memory is usually not full. The reason behind this under-utilization is that there are always segment issues for allocating CPU and memory. Furthermore, the servers underpinning the IaaS is not a good fit for raising trend of heterogeneous computing as adding various new computing devices into servers is not a trivial process.

Hardware resource disaggregated architecture is proposed to address these limits. In this architecture, resources are no longer bound with a server. In other words, resources are collected in pools, such as processor pool, memory pool, and storage pool, and there is no boundary among each class of resources. In this setting, the resources can be distributed in the exact number of request, the unallocated resources can be shutdown. For the following article, we use monolithic architecture as the traditional server design and disaggregated architecture as our target architecture, as illustrated in Fig \ref{fig:architecture}.

While there is an operating system prototype, LegoOS\cite{LegoOS}, designed for the disaggregated architecture. We believe the main purpose of disaggregated architecture is to support cloud computing. Designing a hypervisor to support the IaaS functionality is, therefore, an important thing to be done.

For disaggregated architecture, all hardware computing resources such as processors, memory, and storage are separately located. Though processors and memory are separate, we assume there are still a few amounts of memory stays with the processors. This assumption is reasonable given the network speed still not catches on the processor-memory performance requirement\cite{Network_requirement} and datacenter applications usually show great temporal data locality\cite{NN_load,MapReduce_load}. So in this paper, the memory components in our target architecture include local and remote memory as depicted in Fig \ref{fig:disaggregated architecture}.

Also, we assume a hierarchical memory architecture. As shown in Fig \ref{fig:memory hierarchy}, there are CPU cache, local memory, remote memory, and remote storage. The CPU access latency is shorter as the layer grows up and the capacity is larger as the layer goes down. The interaction between each layer can only happen with the one's upper or lower layer. Cross-layer interaction is not allowed. Though a flat memory architecture is also a possible design to implement the disaggregated architecture, we keep a simple model for better answering our problems.

This work is a pre-step before going into the design of hypervisor as there are some unclear questions about local memory on disaggregated architecture needed to be answered. We focus on local memory since it makes a huge impact in the disaggregated architecture as the main memory stays remotely and has higher latency. The first thing we explore is that how much local memory is enough for running a virtual machine and to achieve comparable performance? Second, for running multiple virtual machines, should the local memory be dedicated or shared? This work uses a simulation approach to evaluate the performance of applications and try to answer the above questions.

We simulate the disaggregated architecture by separating the main memory of a machine into local and remote memory regions. The processes are only allowed to access the local memory region. page fault and page eviction happen if the data is not in the local memory region. The data is first loaded into the remote memory region and then is copied to the local memory region before the following access. A configurable delay is added in the page fault handler to simulate the latency in access remote memory via network.

To sum up, we believe building a hypervisor for disaggregated architecture is a more practical way to explore it. And in the virtualized environment, this paper explores what should be the right local memory design to achieve acceptable performance compared with traditional monolithic architecture.

\section{Background}
In the traditional architecture, programs (for desktop users) or virtual machines (as what IaaS provides) run on top of a monolithic machine where all of the hardware components are bounded within the machine. While this setting has been working well for many years, current trends raise some difficulties to adopt this model. Besides, there are also new techniques to be enabling factors of disaggregated architecture. The following sections discuss the limitation of monolithic servers and then describe the primary concept of resource disaggregation.

\subsection{Monolithic Architecture Limitations}
In this subsection, we discuss three limitations of monolithic architecture: resource under-utilization, poor hardware elasticity, and poor failure handling.

First, in monolithic servers, all the hardware components are bounded within a physical machine. In the context of IaaS, for example, these machine boundaries are not a good fit for providing virtual machines to users. Think about a scenario that a user requests a virtual machine composed of many processors but only a few amounts of memory. This request ends up using most of the processors but only a small portion of memory in a physical machine. It's then not possible for cloud OS to distribute other users' requests to that physical machine as there are not many processors left. Therefore, the memory of the machine is under-utilized. This under-utilization issues could be generalized into any category of hardware resources.

As for poor hardware elasticity, having a boundary on hardware resources makes it extremely hard to add or remove hardware components to a machine. Increasing the server's computation power by adding more processors or memory is not easy to do. Furthermore, this limitation becomes more daunting as specific hardware accelerators are viewed as a future trend these days\cite{Heterogeous_computing}, such as GPGPU\cite{Google_GPGPU}, TPU\cite{TPU}, FPGA\cite{Datacenter_FPGA}. While in a disaggregated setting, any hardware component could be added or remove via attaching or de-attach the HW-blade to other inter-connected hardware components.

Finally, the failure handling for monolithic architecture is that, even though there is a failure on only one hardware component, that failure usually causes the entire system crushes. This is not an appealing outcome as people are expecting hardware component failures could be handled independently without breaking down the whole system. In the cloud provider, this failure model is even severe since the crush of the entire machine will affect multiple users. In the disaggregated architecture, however, the faults of different components could be handled independently as there are loose-coupled.

\subsection{Resource Disaggregation}
The concept of resource disaggregation is that instead of having hardware components bounded within a physical machine, different hardware components should be separate. We could have processors, memory, storages, and I/O devices sit in a different place. These hardware components communicate with each other to provide functionalities that a computer should provide. From a user's perspective, it just sees a machine abstraction carrying out all their computing requests.

Providing machine abstraction by separate hardware components faces some technical challenges\cite{Rack_scale_challenges}. The first one is the efficiency of network communication. Performance degradation can be expected if the network speed is not fast enough. Another challenge is that traditional hardware components cannot work independently and might require some assistance from CPUs. Take DMA for example, though DMA is free of CPU's intervention during data copy, it still needs CPU to do some initialization. The cases are the same in network and storage devices.

Fortunately, current techniques resolve some of the above challenges. We describe some key enabling factors for resource disaggregation. First, faster network communication\cite{R2C2}, RDMA\cite{RDMA}, and InfiniteBand\cite{InfiniteBand} are introduced and become more and more popular recently. Furthermore, hardware devices with the programmability of standalone computing functionality are also emerging\cite{PARDIS, PIM}.

% Figure of local memory architecture
\begin{figure*}[h!]
     \centering
     \captionsetup[subfigure]{position=b}
     \begin{subfigure}[b]{\textwidth}
         \includegraphics[height=.025\textheight,width=\linewidth]{figures/memory_region.png}
         \caption{Separate physical main memory into local (slash-filled) and remote memory resgions.}
         \label{fig:local memory regions}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\textwidth}
         \includegraphics[height=.05\textheight,width=\linewidth]{figures/shared_local_mem.png}
         \caption{The four VMs, blue colered, share the local memory region. It's possible that guest physical address of two VMs end up mapping to the same machine physical address, VM2 and VM3 for example.}
         \label{fig:shared local memory}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\textwidth}
         \includegraphics[height=.05\textheight,width=\linewidth]{figures/dedicated_local_mem.png}
         \caption{The four VMs, blue colered own its own local memory region. These regions are not shared with other VMs.}
         \label{fig:dedicated local memory}
     \end{subfigure}
     \caption{Configuration of local memory region.}
     \label{fig:local memory architecture}
\end{figure*}

\section{Related Work}

\subsection{Prototype Systems}
The datacenter systems have used monolithic architecture for many years where the hardware resources are tightly integrated within one machine. There are some limits on the monolithic architecture including under-utilized resources, weak support for heterogeneous computing, poor failure handling, and hard to achieve hardware resource elasticity. The resource disaggregation architecture is then proposed to tackle these limits. There are some disaggregated architecture proposals include Intel’s Rack Scale Architecture\cite{Intel_RSA}, HP’s “The Machine”\cite{HP_The_Machine}, Facebook’s Disaggregated Rack\cite{FB_disaggregated_rack}, and IBM’s Composable System\cite{IBM_composable_system}.

\subsection{Enabling Techniques}
The key enabling factor of the disaggregated architecture is network techniques that whether network performance can meet the performance requirement of bus-connected processors and memory\cite{Network_requirement}. Some network techniques enable the possibility of the disaggregated architecture: the high-speed and more scalable network techniques such as RDMA\cite{RDMA}, InfiniteBand\cite{InfiniteBand}, and new network stack for rack-scale computers\cite{Network_stack_for_rack}; network hardware devices accessing without the need of attaching to processors\cite{NVM_performance, NVM_performance2}.

\subsection{Softwares for Resource Disaggregation}
As for the software sides to manage resources on disaggregated architecture, there are some techniques to target specific types of resources. Support disaggregated memory with transparently access through processor’s memory instructions\cite{Disaggregated_memory, Disaggregated_memory2}. Another challenging point to resource disaggregation is how to separate memory from processors as most of the architecture have tightly-coupled processors and memory. Moreover, there are also instructions carrying out memory access. LegoOS[18] emulates the separation of processor and memory by trapping all memory accesses, excluding cache access, and redirect them to access remote memory via page fault handler. As for storage management, accessing the remote disks is already a common practice\cite{VMware_virtual_SAN, Amazon_EC2_root_volume}.

\subsection{Previous OSes}
For operating systems, there are some previous works to manage the distributed resource across multiple machines. Ameoba OS\cite{Amoeba_OS} and fos\cite{fos} builds a single image microkernel-based OS to run across multiple physical machines, for example, supporting Infrastructure as a Service (IaaS) in the cloud computing context. The motivation is to release application developers from the complexity of resources management. Having many machines to form a huge resource pool, users should be able to use the resources without considering managing them across machines or worry about the location of machines. For example, in the cloud computing context, developers don’t need to consider how many resources and VMs are required for a certain task.

Both Ameoba OS and fos support allocate resources from multiple machines. One major difference between these two is that, though Ameoba OS supports allocate resources from many machines, once the machine is selected, the process will only use the hardware components on it, e.g., the process cannot use processors and memory on two machines. fos, on the other hand, does allow one application to use resources from different machines. fos provides the resource elasticity functionality by allowing users to simply call a spawn procedure. The machine where the spawned process runs on, either on the same machine or on a different machine, is handled by the fos.

LegoOS\cite{LegoOS} is a prototype operating system specifically targeting on disaggregated architecture. It proposes a splitkernel OS architecture that split the kernel functionalities into several services. As the hardware resources are separate, the managing software is also split. Each service manages its own hardware component and communicates with each other via network. For instance, there are different software components to manage processors, memory, and storage hardware individually. But same as Ameoba OS and fos, LogoOS design a completely new OS and therefore is weak in backward compatibility and lack of flexibility for users to choose different OSes to use.

% Figure of page fault handling
\begin{figure*}[h!]
     \centering
     \includegraphics[width=\linewidth,height=.3\textheight]{figures/page_fault_flow.png}
     \caption{Page fault flow to simulate remote memory and remote storage access.}
     \label{fig:page fault flow}
\end{figure*}

% Figure of physical memory view
\begin{figure*}[h!]
     \centering
     \includegraphics[width=\linewidth]{figures/physical_memory_view.png}
     \caption{Local and remote memory simulation, a physical view.}
     \label{fig:physical memory view}
\end{figure*}

\section{Design}
In this section, we first discuss the application model. In particular, do applications or guest OSes know there are running on top of disaggregated architecture? After introducing the application model, our simulation approach is described.

\subsection{Application Model}
One of our primary reasons for building a hypervisor for disaggregated architecture is it provides the benefits for legacy applications and guest OSes. The software running one traditional architecture is not expected to make any modifications. With the hypervisor, we look forward to more adoption of disaggregated architecture. Moreover, we think it's the systems that need to adapt to the architecture changes rather than applications. Our hypervisor can also guide the disaggregated hardware design, especially the resource disaggregation is at a very beginning stage.

\subsection{Simulation Approach}
We simulate the local and remote memory using the main memory of a machine as illustrated in Fig \ref{fig:local memory regions}. We manage the page table to separate the main memory for simulating local and remote memory on the disaggregated architecture. Only the memory address to the local memory region is valid, accessing other address will cause a page fault. In the page fault handler, we will first check whether the page stays on the remote memory region and copy the data into the local memory region if so. Otherwise, storage access for reading the data into the remote memory region, and then copy the data into local memory region. Local page eviction, page table entry invalidation, and TLB invalidation will also be executed to guarantee correctness. Moreover, we will execute an intentional wait in the page fault handler to simulate the latency for fetching a remote memory via network in the disaggregated architecture. The waiting time is a configurable variable in order to see which level of the latency is acceptable for achieving comparable performance with monolithic architecture. Also, the size of local memory region is changeable in order to do an analysis of what local memory size is a better design for disaggregated architecture.

\subsection{Shared/Dedicated Local Memory}
In this paper, we explore the possibility of providing dedicated local memory to each virtual machine and compare it with the shared local memory policy to see which is a better policy for disaggregated architecture. See Fig \ref{fig:shared local memory} and Fig \ref{fig:dedicated local memory} for illustrations.

For shared local memory, we simply make the whole local memory region shared by multiple virtual machines and use the original page replacement policy implemented in the hypervisor to choose the victim to be evicted. So the victim page might be a page of any virtual machine.

Dedicated local memory, on the other hand, we divided the whole local memory region and assign each sub-region to each virtual machine equally. Whenever a page needs to be evicted, only the page in the sub-region owned by the virtual machine causing the page fault will become a victim. The process will not evict a page in other virtual machines' local memory regions.

\section{Implementation}
We implement our simulation framework by extending KVM hypervisor. The key is how to manipulate page tables and page faults correctly to correctly simulate the behavior of local and remote memory of disaggregated architecture. Before going to the implementation details, we describe some terminologies related to address translation that will show in the following article.

\subsection{Address Translation in a Virtualized Environment}
In a virtualized environment, there are three software layers: guest applications, guest OSes, and hypervisor. There are also three address spaces: guest virtual address (GVA), guest physical address (GPA), and machine physical address (MPA). GVA is the address issued by executing guest applications and guest OSes. And the guest's page table will translate the GVA into GPA. While in a non-virtualized environment, the GPA is actually the final physical address and is valid to be used to access memory. In a virtualized environment, on the other hand, the GPA needs a further translate into MPA and then is able to access the memory. The translation from GPA to MPA is handled by the page table maintained by the hypervisor.

\subsection{Simulating Local/Remote Memory}
In this section, we descripe the implementation of our simulation framework. The key idea is to separate the main memory of a server into local and remote memory region. It's worthy of noting that the region separation is from the MPA's viewpoint. Guest applications and OSes are allowed to access the local memory region, otherwise, a page fault comes. If local memory is not enough, a page in the local memory region is evicted into remote memory regions such that a memory request can be made to local memory region. In the local page eviction process, the corresponding page table entry (PTE) is updated to point the remote memory region and is marked unreadable to cause a further page fault. Corresponding TLB is flushed to assure correctness.

The detailed page fault flow is depicted in Fig \ref{fig:page fault flow}. When a page fault happens, this implies that remote memory is accessed. We do a wait in the fault handler to simulate the remote memory access latency. If the page is not in the remote memory region, a storage access is made and incurs a wait to simulate remote storage access. Either the page in remote memory region or need a storage access, the data will end up being cached in the remote memory region. Then the page is copied to local region and followed by a PTE updated to make future access hit the page in local memory region as well a TLB flush. Similar to the previous paragraph described, if a page eviction happens, it comes with a PTE update and TLB flushed for that victim page.

It's worth noting that \ref{fig:local memory architecture} is a logic view of the local and remote memory layout. The physical view of memory layout should be \ref{fig:physical memory view}. The upper figure shows that the local memory is full. The colored squares represent pages in local memory. When there is one more allocation after local memory is full, a victim, grey-colored, is swapped out, then an allocation, red-colored, is fulfilled.
                                         
\subsection{KVM/Qemu Interfaces}                         
There are two controlling factors for our simulation disaggregated memory simulation: local memory size and network delay. We modify the KVM interface to accept parameters for setting these two factors. A new IOCTL command, KVM\_ENABLE\_DSAG\_MEM\_SIM, is introduced and it takes a struct including local memory size and network delays as an argument. Also, the Qemu side is modified to send this new IOCTL command and pass an argument to KVM. Our modification is based on Linux v5.0.0 and Qemu v3.1.0.

\section{Evaluation}

\subsection{Plan}
In the evaluation section, we try to answer two questions: 1) how much local memory is required to prevent significant performance drop? 2) should the local memory be dedicated or shared between guest OSes? After understanding these two questions, we can make a hint about how a hypervisor does local memory management for disaggregated architecture.

For the first question, the purpose is to know whether the disaggregated architecture can still achieve comparable performance even most of the memory stay remotely. We run workloads on one virtual machine once at a time and vary the local memory size for each run. The baseline is to run the workloads using all memory as local memory. We try to find a size setting that can achieve ~90\% of the baseline performance.

As for the second question, we try to know that if local memory is not enough for multiple virtual machines, will using shared memory compromise the performance isolation between virtual machines too much. And whether using dedicated memory cause under-utilization issue. For the experiment, we run workloads on multiple virtual machines. The primary control variable is enabling shared or dedicated memory and to observe the performance variation.

% System configuration table
\begin{table}[h!]
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{|l|l|l|l|}
        \hline
               & HW Configuration               & Host  & Guest \\ \hline
        CPU    & Intel Core i7-6700T @ 2.80 GHz & 8     & 4     \\ \hline
        Memory & DDR4 @ 2400 MT/s               & 32 GB & 4 GB  \\ \hline
        \end{tabular}
    }
    \caption{System configurations}
    \label{table:system configuration}
\end{table}

% Figure of CIFAR-10 result
\begin{figure}[h!]
     \centering
     \includegraphics[width=\linewidth]{figures/cifar10_result.png}
     \caption{Tensorflow CIFAR-10 result}
     \label{fig:cifar-10 result}
\end{figure}

\subsection{Preliminary Result}                            
We use a single desktop to simulate the disaggregated memory. The system configuration is shown in Table \ref{table:system configuration}. We use TensorFlow to train CIFAR-10 dataset \cite{Tensorflow, CIFAR-10} as our benchmark. All the results run in a virtualized environment, i.e., within a guest OS. The baseline is to run the benchmark on an unmodified hypervisor, which represents benchmarking a traditional architecture. The other results run our modified KVM, which simulates the disaggregated memory behaviors. The result is depicted in Fig \ref{fig:cifar-10 result} and is normalized against the baseline. The x-axis is the two controlling factors, network delay in microsecond unit and local memory size in MB.
                                         
The trend of the CIFAR-10 result with respect to network delay is as expected, the time increases as the network delay increases. But the local memory change doesn't show significant differences. And all of the results could actually be viewed as within acceptable error range, around 5\% differences, and have barely discrimination. The reason why there is only a little performance is still under investigation. We provide some observations and possible reasons in the following subsection.
                                         
\subsection{Analysis}                                 
We dump some log to trace the number of pages allocated and those that are swapped out during the guest execution. Before the CIFAR-10 is running, the number of pages allocated is around three thousand. After running the CIFAR-10, the number of pages allocated is increased to four thousand. These pages in total are only 16 MB. This means that a guest only uses around 16 MB memory even after running the CIFAR-10. This is a number that far lower than we expect. The other way to check the memory usage is via Linux's debugfs, via 'cat /proc/PID/statm'. This shows the guest uses around 600,000 pages before running CIFAR-10 and 1,050,000 pages after running CIFAR-10, which are 2.4 GB and 4.2 GB respectively.                                         
The observation shows that there is a large number of mismatching in our log and Linux's record. Our log and implementation are mainly on one entry of the guest's memory page fault. We guess that there might be other entries of the guest's memory fault and we didn't catch it. For instance, MMIO and shared memory might be going to another entry since they require different page fault handling.
                                         
Understanding memory virtualization is a huge task. And understand KVM's memory virtualization implementation is even difficult since there is a lot of optimizations and different paths. We'll work more on understanding the KVM's memory virtualization mechanism and have a better implementation to finish the disaggregated memory simulation.
                                         
\section{Conclusion}                                 
In this work, we use a simulation approach to explore the local memory design for disaggregated architecture, in particular, this paper targets to decide what is the appropriate local size. The implementation is built on top of KVM and most of the modification is on the two-dimension page fault handler. There are still some issues in the implementation so we don't have a good conclusion for the problems we try to answer. But the core skeleton of the simulation framework is finished. I believe it's as well crucial to illustrate what are the future works to we will follow-up.
                                         
\section{Future Works}                              
                                         
\subsection{Remote Page Pre-fetching}                       
Data pre-fetching is broadly used for fetching data into CPU caches. In traditional architecture, the pre-fetching for lower memory hierarchy, fetching data from storage to memory, is not so popular for two reasons. First, the cost of storage access is expensive. If the pre-fetching policy makes a wrong decision, the expensive storage access will then be a waste. Second, the memory capacity is much larger than cache so the demands of pre-fetching data into memory are not crucial.
                                         
In disaggregated architecture, on the other hand, the above two reasons are no longer exists. In disaggregated architecture, a pre-fetching from remote memory to local memory is network access, which is much faster than storage access in today's techniques. And the local memory capacity in disaggregated architecture is supposed to be small so pre-fetching techniques might be something to reconsider.
                                         
\subsection{Local Page Replacement Polity}                    
When the local memory region is full, we pick a victim to be swapped out in a random manner. Though different page replacement policies might have different determination on the local memory size, the system-level implication paper \cite{Disaggregated_memory2} has a conclusion that using random policy might also be appropriate in the disaggregated setting. But still, there are some workloads that suffer from the random policy. So it's still worth investigating the page replacement design.

\subsection{Direct Remote Memory Computation}                   
When the network speed becomes faster and faster. It might be possible that some data operation can be done on the remote memory directly without bringing the data to local memory first. Some emerging techniques such as process-in-memory (PIM) will underpin this statement. The future work will include exploring the state-of-the-art PIM techniques, and figure what kind of workloads is the best fit for this direct remote memory operation. If we can have a conclusion about the appropriate workloads for PIM, the following problems will be how to dynamically detect or adjust the memory operations to use PIM for direct remote memory operations.
                                         
\subsection{Benchmarking Real-world Workloads}                  
As the disaggregated architecture will be most adopted in datacenter. More datacenter workloads and benchmarks analysis are crucial and more close to real-world's attention. So our evaluation should include datacenter workloads. The network requirement paper \cite{Network_requirement} gives comprehensive workloads for further benchmarking and analyzing.

%latex here should be the name of your bibtex file minus '.bib'
\bibliography{paper}{}
\bibliographystyle{plain}

\end{document}
