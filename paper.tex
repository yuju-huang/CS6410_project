\documentclass[twocolumn]{article}
\usepackage{cite}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{url}
\title{Explore Local Memory Design for Building Hypervisor on Resource Disaggregated Architecture}
\author{Yu-Ju Huang}
\date{Cornell University}
\begin{document}
\maketitle

\begin{abstract}
Resource disaggregation is a new trend for building datacenter as it solves many limitations of traditional monolithic server architecture. Building a hypervisor can provide full compatibility to enable legacy applications to run on disaggregated architecture. Furthermore, virtualization functionality is usually an important feature a datacenter should possess, to support Infrastructure as a Service (IaaS) for instance.

This work explores the local memory design choices for building a hypervisor on disaggregated architecture. As in disaggregated architecture, the main memory stays remotely instead of locating with processors, the management of local memory will be the key system performance. We try to know how much local memory is enough for running a virtual machine to achieve comparable performance in a monolithic architecture. Another unclear thing is that, for running multiple virtual machines, should the local memory be dedicated or shared.

\end{abstract}

% Figure of monolithic architecture and disaggregared architecture
\iffalse
\begin{figure*}[h!]
  \subcaptionbox{Monolithic architecture, hardware components are bounded within a machine.\label{fig:monolithic}}[.3\textwidth]{\includegraphics[height=.2\textheight,width=.3\textwidth]{figures/monolithic.png}}
  \subcaptionbox{Disaggregated architecture, hardware components are separate: there are CPU-blade, memory-blade, storage-blade. We also assume there is local memory located with CPU.\label{fig:disaggregated}}[.7\textwidth]{\includegraphics[height=.2\textheight,width=.7\textwidth]{figures/disaggregated.png}}
  \caption{Monolithic architecture and disaggregated architecture.}
  \label{fig:architecture}
\end{figure*}
\fi

\begin{figure*}[h!]
     \centering
     \captionsetup[subfigure]{position=b}
     \begin{subfigure}[b]{0.28\textwidth}
         \centering
         \includegraphics[height=.15\textheight]{figures/monolithic.png}
         \caption{Monolithic architecture, hardware components are bounded within a machine.}
         \label{fig:monolithic architecture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.68\textwidth}
         \centering
         \includegraphics[height=.15\textheight]{figures/disaggregated.png}
         \caption{Disaggregated architecture, hardware components are separate: there are CPU-blade, memory-blade, storage-blade. We also assume there is local memory located with CPU}
         \label{fig:disaggregated architecture}
     \end{subfigure}
     \caption{Monolithic architecture and disaggregated architecture.}
     \label{fig:architecture}
\end{figure*}

% Figure for memory hierarchy
\begin{figure}[h!]
     \centering
     \includegraphics[width=\linewidth,height=.15\textheight]{figures/memory_hierarchy.png}
     \caption{Memory hierarchy of disaggregated architecture.}
     \label{fig:memory hierarchy}
\end{figure}

\section{Introduction}
Cloud computing has been popular since 2006. Recently, there are growing interests in investigating the limits IaaS (Infrastructure as a Service) has faced. The IaaS is built on top of many servers and managed by a cloud operating system. Cloud operating system accepts demands from users and allocates resources to them. While the interest in adopting these services is still growing, resource utilization of the CPU and memory is usually not full. The reason behind this under-utilization is that there are always segment issues for allocating CPU and memory. Furthermore, the servers underpinning the IaaS is not a good fit for raising trend of heterogeneous computing as adding various new computing devices into servers is not a trivial process.

Hardware resource disaggregated architecture is proposed to address these limits. In this architecture, resources are no longer bound with a server. In other words, resources are collected in pools, such as processor pool, memory pool, and storage pool, and there is no boundary among each class of resources. In this setting, the resources can be distributed in the exact number of request, the unallocated resources can be shutdown. For the following article, we use monolithic architecture as the traditional server design and disaggregated architecture as our target architecture, as illustrated in Fig \ref{fig:architecture}.

While there is an operating system prototype, LegoOS\cite{LegoOS}, designed for the disaggregated architecture. We believe the main purpose of disaggregated architecture is to support cloud computing. Designing a hypervisor to support the IaaS functionality is, therefore, an important thing to be done.

For disaggregated architecture, all hardware computing resources such as processors, memory, and storage are separately located. Though processors and memory are separate, we assume there are still a few amounts of memory stays with the processors. This assumption is reasonable given the network speed still not catches on the processor-memory performance requirement\cite{Network_requirement} and datacenter applications usually show great temporal data locality\cite{NN_load,MapReduce_load}. So in this paper, the memory components in our target architecture include local and remote memory as depicted in Fig \ref{fig:disaggregated architecture}.

Also, we assume a hierarchical memory architecture. As shown in Fig \ref{fig:memory hierarchy}, there are CPU cache, local memory, remote memory, and remote storage. The CPU access latency is shorter as the layer grows up and the capacity is larger as the layer goes down. The interaction between each layer can only happen with the one's upper or lower layer. Cross-layer interaction is not allowed. Though a flat memory architecture is also a possible design to implement the disaggregated architecture, we keep a simple model for better answering our problems.

This work is a pre-step before going into the design of hypervisor as there are some unclear questions about local memory on disaggregated architecture needed to be answered. We focus on local memory since it makes a huge impact in the disaggregated architecture as the main memory stays remotely and has higher latency. The first thing we explore is that how much local memory is enough for running a virtual machine and to achieve comparable performance? Second, for running multiple virtual machines, should the local memory be dedicated or shared? This work uses a simulation approach to evaluate the performance of applications and try to answer the above questions.

We simulate the disaggregated architecture by separating the main memory of a machine into local and remote memory regions. The processes are only allowed to access the local memory region. page fault and page eviction happen if the data is not in the local memory region. The data is first loaded into the remote memory region and then is copied to the local memory region before the following access. A configurable delay is added in the page fault handler to simulate the latency in access remote memory via network.

To sum up, we believe building a hypervisor for disaggregated architecture is a more practical way to explore it. And in the virtualized environment, this paper explores what should be the right local memory design to achieve acceptable performance compared with traditional monolithic architecture.

\section{Background}
In the traditional architecture, programs (for desktop users) or virtual machines (as what IaaS provides) run on top of a monolithic machine where all of the hardware components are bounded within the machine. While this setting has been working well for many years, current trends raise some difficulties to adopt this model. Besides, there are also new techniques to be enabling factors of disaggregated architecture. The following sections discuss the limitation of monolithic servers and then describe the primary concept of resource disaggregation.

\subsection{Monolithic Architecture Limitations}
In this subsection, we discuss three limitations of monolithic architecture: resource under-utilization, poor hardware elasticity, and poor failure handling.

First, in monolithic servers, all the hardware components are bounded within a physical machine. In the context of IaaS, for example, these machine boundaries are not a good fit for providing virtual machines to users. Think about a scenario that a user requests a virtual machine composed of many processors but only a few amounts of memory. This request ends up using most of the processors but only a small portion of memory in a physical machine. It's then not possible for cloud OS to distribute other users' requests to that physical machine as there are not many processors left. Therefore, the memory of the machine is under-utilized. This under-utilization issues could be generalized into any category of hardware resources.

As for poor hardware elasticity, having a boundary on hardware resources makes it extremely hard to add or remove hardware components to a machine. Increasing the server's computation power by adding more processors or memory is not easy to do. Furthermore, this limitation becomes more daunting as specific hardware accelerators are viewed as a future trend these days\cite{Heterogeous_computing}, such as GPGPU\cite{Google_GPGPU}, TPU\cite{TPU}, FPGA\cite{Datacenter_FPGA}. While in a disaggregated setting, any hardware component could be added or remove via attaching or de-attach the HW-blade to other inter-connected hardware components.

Finally, the failure handling for monolithic architecture is that, even though there is a failure on only one hardware component, that failure usually causes the entire system crushes. This is not an appealing outcome as people are expecting hardware component failures could be handled independently without breaking down the whole system. In the cloud provider, this failure model is even severe since the crush of the entire machine will affect multiple users. In the disaggregated architecture, however, the faults of different components could be handled independently as there are loose-coupled.

\subsection{Resource Disaggregation}
The concept of resource disaggregation is that instead of having hardware components bounded within a physical machine, different hardware components should be separate. We could have processors, memory, storages, and I/O devices sit in a different place. These hardware components communicate with each other to provide functionalities that a computer should provide. From a user's perspective, it just sees a machine abstraction carrying out all their computing requests.

Providing machine abstraction by separate hardware components faces some technical challenges\cite{Rack_scale_challenges}. The first one is the efficiency of network communication. Performance degradation can be expected if the network speed is not fast enough. Another challenge is that traditional hardware components cannot work independently and might require some assistance from CPUs. Take DMA for example, though DMA is free of CPU's intervention during data copy, it still needs CPU to do some initialization. The cases are the same in network and storage devices.

Fortunately, current techniques resolve some of the above challenges. We describe some key enabling factors for resource disaggregation. First, faster network communication\cite{R2C2}, RDMA\cite{RDMA}, and InfiniteBand\cite{InfiniteBand} are introduced and become more and more popular recently. Furthermore, hardware devices with the programmability of standalone computing functionality are also emerging\cite{PARDIS, PIM}.

% Figure of local memory architecture
\begin{figure*}[h!]
     \centering
     \captionsetup[subfigure]{position=b}
     \begin{subfigure}[b]{\textwidth}
         \includegraphics[height=.025\textheight,width=\linewidth]{figures/memory_region.png}
         \caption{Separate physical main memory into local (slash-filled) and remote memory resgions.}
         \label{fig:local memory regions}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\textwidth}
         \includegraphics[height=.05\textheight,width=\linewidth]{figures/shared_local_mem.png}
         \caption{The four VMs, blue colered, share the local memory region. It's possible that guest physical address of two VMs end up mapping to the same machine physical address, VM2 and VM3 for example.}
         \label{fig:shared local memory}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\textwidth}
         \includegraphics[height=.05\textheight,width=\linewidth]{figures/dedicated_local_mem.png}
         \caption{The four VMs, blue colered own its own local memory region. These regions are not shared with other VMs.}
         \label{fig:dedicated local memory}
     \end{subfigure}
     \caption{Configuration of local memory region.}
     \label{fig:local memory architecture}
\end{figure*}

\section{Related Work}

\subsection{Prototype Systems}
The datacenter systems have used monolithic architecture for many years where the hardware resources are tightly integrated within one machine. There are some limits on the monolithic architecture including under-utilized resources, weak support for heterogeneous computing, poor failure handling, and hard to achieve hardware resource elasticity. The resource disaggregation architecture is then proposed to tackle these limits. There are some disaggregated architecture proposals include Intel’s Rack Scale Architecture\cite{Intel_RSA}, HP’s “The Machine”\cite{HP_The_Machine}, Facebook’s Disaggregated Rack\cite{FB_disaggregated_rack}, and IBM’s Composable System\cite{IBM_composable_system}.

\subsection{Enabling Techniques}
The key enabling factor of the disaggregated architecture is network techniques that whether network performance can meet the performance requirement of bus-connected processors and memory\cite{Network_requirement}. Some network techniques enable the possibility of the disaggregated architecture: the high-speed and more scalable network techniques such as RDMA\cite{RDMA}, InfiniteBand\cite{InfiniteBand}, and new network stack for rack-scale computers\cite{Network_stack_for_rack}; network hardware devices accessing without the need of attaching to processors\cite{NVM_performance, NVM_performance2}.

\subsection{Softwares for Resource Disaggregation}
As for the software sides to manage resources on disaggregated architecture, there are some techniques to target specific types of resources. Support disaggregated memory with transparently access through processor’s memory instructions\cite{Disaggregated_memory, Disaggregated_memory2}. Another challenging point to resource disaggregation is how to separate memory from processors as most of the architecture have tightly-coupled processors and memory. Moreover, there are also instructions carrying out memory access. LegoOS[18] emulates the separation of processor and memory by trapping all memory accesses, excluding cache access, and redirect them to access remote memory via page fault handler. As for storage management, accessing the remote disks is already a common practice\cite{VMware_virtual_SAN, Amazon_EC2_root_volume}.

\subsection{Previous OSes}
For operating systems, there are some previous works to manage the distributed resource across multiple machines. Ameoba OS\cite{Amoeba_OS} and fos\cite{fos} builds a single image microkernel-based OS to run across multiple physical machines, for example, supporting Infrastructure as a Service (IaaS) in the cloud computing context. The motivation is to release application developers from the complexity of resources management. Having many machines to form a huge resource pool, users should be able to use the resources without considering managing them across machines or worry about the location of machines. For example, in the cloud computing context, developers don’t need to consider how many resources and VMs are required for a certain task.

Both Ameoba OS and fos support allocate resources from multiple machines. One major difference between these two is that, though Ameoba OS supports allocate resources from many machines, once the machine is selected, the process will only use the hardware components on it, e.g., the process cannot use processors and memory on two machines. fos, on the other hand, does allow one application to use resources from different machines. fos provides the resource elasticity functionality by allowing users to simply call a spawn procedure. The machine where the spawned process runs on, either on the same machine or on a different machine, is handled by the fos.

LegoOS\cite{LegoOS} is a prototype operating system specifically targeting on disaggregated architecture. It proposes a splitkernel OS architecture that split the kernel functionalities into several services. As the hardware resources are separate, the managing software is also split. Each service manages its own hardware component and communicates with each other via network. For instance, there are different software components to manage processors, memory, and storage hardware individually. But same as Ameoba OS and fos, LogoOS design a completely new OS and therefore is weak in backward compatibility and lack of flexibility for users to choose different OSes to use.

% Figure of page fault handling
\begin{figure*}[h!]
     \centering
     \includegraphics[width=\linewidth,height=.3\textheight]{figures/page_fault_flow.png}
     \caption{Page fault flow to simulate remote memory and remote storage access.}
     \label{fig:page fault flow}
\end{figure*}

\section{Design}
In this section, we first discuss the application model. In particular, do applications or guest OSes know there are running on top of disaggregated architecture? After introducing the application model, our simulation approach is described.

\subsection{Application Model}
One of our primary reasons for building a hypervisor for disaggregated architecture is it provides the benefits for legacy applications and guest OSes. The software running one traditional architecture is not expected to make any modifications. With the hypervisor, we look forward to more adoption of disaggregated architecture. Moreover, we think it's the systems that need to adapt to the architecture changes rather than applications. Our hypervisor can also guide the disaggregated hardware design, especially the resource disaggregation is at a very beginning stage.

\subsection{Simulation Approach}
We simulate the local and remote memory using the main memory of a machine as illustrated in Fig \ref{fig:local memory regions}. We manage the page table to separate the main memory for simulating local and remote memory on the disaggregated architecture. Only the memory address to the local memory region is valid, accessing other address will cause a page fault. In the page fault handler, we will first check whether the page stays on the remote memory region and copy the data into the local memory region if so. Otherwise, storage access for reading the data into the remote memory region, and then copy the data into local memory region. Local page eviction, page table entry invalidation, and TLB invalidation will also be executed to guarantee correctness. Moreover, we will execute an intentional wait in the page fault handler to simulate the latency for fetching a remote memory via network in the disaggregated architecture. The waiting time is a configurable variable in order to see which level of the latency is acceptable for achieving comparable performance with monolithic architecture. Also, the size of local memory region is changeable in order to do an analysis of what local memory size is a better design for disaggregated architecture.

\subsection{Shared/Dedicated Local Memory}
In this paper, we explore the possibility of providing dedicated local memory to each virtual machine and compare it with the shared local memory policy to see which is a better policy for disaggregated architecture. See Fig \ref{fig:shared local memory} and Fig \ref{fig:dedicated local memory} for illustrations.

For shared local memory, we simply make the whole local memory region shared by multiple virtual machines and use the original page replacement policy implemented in the hypervisor to choose the victim to be evicted. So the victim page might be a page of any virtual machine.

Dedicated local memory, on the other hand, we divided the whole local memory region and assign each sub-region to each virtual machine equally. Whenever a page needs to be evicted, only the page in the sub-region owned by the virtual machine causing the page fault will become a victim. The process will not evict a page in other virtual machines' local memory regions.

\section{Implementation}
We implement our simulation framework by extending Xen hypervisor. The key is how to manipulate page tables and page faults correctly to correctly simulate the behavior of local and remote memory of disaggregated architecture. Before going to the implementation details, we describe some terminologies related to address translation that will show in the following article.

\subsection{Address Translation in a Virtualized Environment}
In a virtualized environment, there are three software layers: guest applications, guest OSes, and hypervisor. There are also three address spaces: guest virtual address (GVA), guest physical address (GPA), and machine physical address (MPA). GVA is the address issued by executing guest applications and guest OSes. And the guest's page table will translate the GVA into GPA. While in a non-virtualized environment, the GPA is actually the final physical address and is valid to be used to access memory. In a virtualized environment, on the other hand, the GPA needs a further translate into MPA and then is able to access the memory. The translation from GPA to MPA is handled by the page table maintained by the hypervisor.

\subsection{Simulating Local/Remote Memory}
The key idea is to separate the main memory of a server into local and remote memory region. It's worthy of noting that the region separation is from the MPA's viewpoint. Guest applications and OSes are allowed to access the local memory region, otherwise, a page fault comes. If local memory is not enough, a page in the local memory region is evicted into remote memory regions such that a memory request can be made to local memory region. In the local page eviction process, the corresponding page table entry (PTE) is updated to point the remote memory region and is marked unreadable to cause a further page fault. Corresponding TLB is flushed to assure correctness.

The detailed page fault flow is depicted in Fig \ref{fig: page fault flow}. When a page fault happens, this implies that remote memory is accessed. We do a wait in the fault handler to simulate the remote memory access latency. If the page is not in the remote memory region, a storage access is made and incurs a wait to simulate remote storage access. Either the page in remote memory region or need a storage access, the data will end up being cached in the remote memory region. Then the page is copied to local region and followed by a PTE updated to make future access hit the page in local memory region as well a TLB flush. Similar to the previous paragraph described, if a page eviction happens, it comes with a PTE update and TLB flushed for that victim page.

\section{Evaluation}
In the evaluation section, we try to answer two questions: 1) how much local memory is required to prevent significant performance drop? 2) should the local memory be dedicated or shared between guest OSes? After understanding these two questions, we can make a hint about how a hypervisor does local memory management for disaggregated architecture.

For the first question, the purpose is to know whether the disaggregated architecture can still achieve comparable performance even most of the memory stay remotely. We run workloads on one virtual machine once at a time and vary the local memory size for each run. The baseline is to run the workloads using all memory as local memory. We try to find a size setting that can achieve ~90\% of the baseline performance.

As for the second question, we try to know that if local memory is not enough for multiple virtual machines, will using shared memory compromise the performance isolation between virtual machines too much. And whether using dedicated memory cause under-utilization issue. For the experiment, we run workloads on multiple virtual machines. The primary control variable is enabling shared or dedicated memory and to observe the performance variation.

%latex here should be the name of your bibtex file minus '.bib'
\bibliography{paper}{}
\bibliographystyle{plain}

\end{document}
